{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_names = ['2014', '2015', '2016', '2017', '2018', '2019']\n",
    "gws = ['3', '5', '10']\n",
    "leagues = ['EPL', 'La_liga', 'Bundesliga', 'Serie_A', 'Ligue_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_understat(payload):\n",
    "    #Build request using url, headers (mimicking what Firefox does normally)\n",
    "    #Works best with verify=True as you won't get the ssl errors. Payload is \n",
    "    #taylored for each request\n",
    "    url = 'https://understat.com/main/getPlayersStats/'\n",
    "    headers = {'content-type':'application/json; charset=utf-8',\n",
    "    'Host': 'understat.com',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko/20100101 Firefox/73.0',\n",
    "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
    "    'X-Requested-With': 'XMLHttpRequest',\n",
    "    'Content-Length': '39',\n",
    "    'Origin': 'https: // understat.com',\n",
    "    'Connection': 'keep - alive',\n",
    "    'Referer': 'https: // understat.com / league / EPL'\n",
    "    }\n",
    "    response = requests.post(url, data=payload, headers = headers, verify=True)\n",
    "    response_json = response.json()\n",
    "    inner_wrapper = response_json['response']\n",
    "    json_player_data = inner_wrapper['players']\n",
    "    return json_player_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(player_df, weeks):\n",
    "    # Get rid of the columns that we don't care about\n",
    "    #player_df.drop(['yellow_cards','red_cards', 'xGChain','xGBuildup','games','time'], axis=1, inplace=True)\n",
    "    player_df  = player_df.rename(columns={'goals':'goals_'+weeks,'xG':'xG_'+weeks,'assists':'assists_'+weeks, 'xA':'xA_'+weeks, 'shots':'shots_'+weeks, 'key_passes':\n",
    "        'key_passes_'+weeks,'npg':'npg_'+weeks,'npxG':'npxG_'+weeks})\n",
    "    \n",
    "    return(player_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gw_data(season , league,  no_of_gw):\n",
    "#     Create Pandas dataframes from each html table\n",
    "    print('Getting data for last {} matches'.format(no_of_gw))\n",
    "    json_player_data = scrape_understat({'league':'EPL', 'season':season, 'n_last_matches': no_of_gw})\n",
    "    gw_table = pd.DataFrame(json_player_data)\n",
    "    gw_df = clean_df(gw_table,'3wks')\n",
    "    #Replace Position indentifiers with something more useful\n",
    "    gw_df['position'] = gw_df['position'].str.slice(0,1)\n",
    "    position_map = {'D':'DEF', 'F':'FWD', 'M':'MID', 'G':'GK', 'S':'FWD'}\n",
    "    gw_df = gw_df.replace({'position': position_map})\n",
    "    gw_df.to_csv(r'C:\\Users\\Asus\\PycharmProjects\\Understat Data Scraper\\Data\\Player_Data\\gw_data\\last_{}_gw_data.csv'.format(no_of_gw), encoding='utf-8', index=False)\n",
    "    print('last {} matches csv data written'.format(no_of_gw))\n",
    "    return gw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for last 3 matches\n",
      "last 3 matches csv data written\n",
      "Getting data for last 5 matches\n",
      "last 5 matches csv data written\n",
      "Getting data for last 10 matches\n",
      "last 10 matches csv data written\n"
     ]
    }
   ],
   "source": [
    "last_3_gw_data_EPL = gw_data(season_names[-1], leagues[0], gws[0])\n",
    "last_5_gw_data_EPL = gw_data(season_names[-1], leagues[0], gws[1])\n",
    "last_10_gw_data_EPL = gw_data(season_names[-1], leagues[0], gws[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_data(season, league):\n",
    "    print('Getting data for {} season'.format(season))\n",
    "    json_player_data = scrape_understat({'league': league, 'season':season})\n",
    "    season_table = pd.DataFrame(json_player_data)\n",
    "    season_df = clean_df(season_table, 'season')\n",
    "    season_df.to_csv(r'C:\\Users\\Asus\\PycharmProjects\\Understat Data Scraper\\Data\\Player_Data\\season_data\\{}_whole_season_data.csv'.format(season), encoding='utf-8', index=False)\n",
    "    print('csv file for {} season written'.format(season))\n",
    "    return season_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for 2014 season\n",
      "csv file for 2014 season written\n",
      "Getting data for 2015 season\n",
      "csv file for 2015 season written\n",
      "Getting data for 2016 season\n",
      "csv file for 2016 season written\n",
      "Getting data for 2017 season\n",
      "csv file for 2017 season written\n",
      "Getting data for 2018 season\n",
      "csv file for 2018 season written\n",
      "Getting data for 2019 season\n",
      "csv file for 2019 season written\n"
     ]
    }
   ],
   "source": [
    "season_1415 = season_data(season_names[0], leagues[0])\n",
    "season_1516 = season_data(season_names[1], leagues[0])\n",
    "season_1617 = season_data(season_names[2], leagues[0])\n",
    "season_1718 = season_data(season_names[3], leagues[0])\n",
    "season_1819 = season_data(season_names[4], leagues[0])\n",
    "season_1920 = season_data(season_names[5], leagues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'season_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a05c8ed08cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mseason_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'season_df' is not defined"
     ]
    }
   ],
   "source": [
    "season_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1718"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_1920"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
